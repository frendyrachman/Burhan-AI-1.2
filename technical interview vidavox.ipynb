{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping from sunnah.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Fungsi untuk mendapatkan data hadis dari sebuah URL buku\n",
    "def get_hadiths_from_book(url):\n",
    "    print(f\"Mengakses URL buku: {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    hadiths_data = []\n",
    "    \n",
    "    # Ambil judul buku\n",
    "    book_title_element = soup.find('div', class_='book_page_english_name')\n",
    "    book_title = book_title_element.text.strip() if book_title_element else \"Judul Tidak Ditemukan\"\n",
    "    print(f\"Judul buku ditemukan: {book_title}\")\n",
    "    \n",
    "    # Loop melalui setiap hadis\n",
    "    hadith_containers = soup.find_all('div', class_='actualHadithContainer')\n",
    "    print(f\"Menemukan {len(hadith_containers)} hadis dalam buku ini.\")\n",
    "    \n",
    "    for hadith_container in hadith_containers:\n",
    "        # Ambil narator hadis\n",
    "        narrator = hadith_container.find('div', class_='hadith_narrated')\n",
    "        narrator_text = narrator.text.strip() if narrator else \"N/A\"\n",
    "        \n",
    "        # Ambil teks hadis\n",
    "        hadith_text = hadith_container.find('div', class_='text_details')\n",
    "        hadith_text = hadith_text.text.strip() if hadith_text else \"N/A\"\n",
    "        \n",
    "        # Ambil referensi hadis\n",
    "        reference = hadith_container.find('div', class_='hadith_reference_sticky')\n",
    "        reference_text = reference.text.strip() if reference else \"N/A\"\n",
    "        \n",
    "        # Simpan data ke dalam list\n",
    "        hadiths_data.append({\n",
    "            'Rawi': url.split('/')[-2].capitalize(),  # Nama periwayat dari URL\n",
    "            'Chapter': book_title,  # Judul buku\n",
    "            'Reference': reference_text,  # Referensi hadis\n",
    "            'Narator': narrator_text,  # Narator hadis\n",
    "            'Hadiths Text': hadith_text  # Teks hadis\n",
    "        })\n",
    "    \n",
    "    print(f\"Berhasil mengumpulkan {len(hadiths_data)} hadis dari buku ini.\\n\")\n",
    "    return hadiths_data\n",
    "\n",
    "# Fungsi untuk mendapatkan semua buku dari sebuah periwayat\n",
    "def get_books_from_author(base_url):\n",
    "    books_data = []\n",
    "    book_number = 1\n",
    "    \n",
    "    print(f\"\\nMemulai scraping untuk periwayat: {base_url}\")\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}/{book_number}\"\n",
    "        print(f\"Mengecek URL buku: {url}\")\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Jika halaman tidak ditemukan, hentikan loop\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Buku dengan nomor {book_number} tidak ditemukan. Menghentikan proses untuk periwayat ini.\\n\")\n",
    "            break\n",
    "        \n",
    "        # Ambil data hadis dari buku tersebut\n",
    "        books_data.extend(get_hadiths_from_book(url))\n",
    "        book_number += 1\n",
    "    \n",
    "    print(f\"Total {len(books_data)} hadis ditemukan untuk periwayat {base_url}.\\n\")\n",
    "    return books_data\n",
    "\n",
    "# List URL periwayat\n",
    "authors_urls = [\n",
    "    \"https://sunnah.com/bukhari\",\n",
    "    \"https://sunnah.com/muslim\",\n",
    "    \"https://sunnah.com/nasai\",\n",
    "    \"https://sunnah.com/abudawud\",\n",
    "    \"https://sunnah.com/ibnmajah\",\n",
    "    \"https://sunnah.com/malik\",\n",
    "    \"https://sunnah.com/ahmad\",\n",
    "    \"https://sunnah.com/riyadussalihin\",\n",
    "    \"https://sunnah.com/adab\",\n",
    "    \"https://sunnah.com/shamail\",\n",
    "    \"https://sunnah.com/mishkat\",\n",
    "    \"https://sunnah.com/hisn\"\n",
    "]\n",
    "\n",
    "# Kumpulkan semua data hadis\n",
    "all_hadiths = []\n",
    "for author_url in authors_urls:\n",
    "    print(f\"\\n=== Memulai scraping untuk periwayat: {author_url} ===\")\n",
    "    all_hadiths.extend(get_books_from_author(author_url))\n",
    "\n",
    "# Buat dataframe dari data yang dikumpulkan\n",
    "df = pd.DataFrame(all_hadiths, columns=['Rawi', 'Chapter', 'Reference', 'Narator', 'Hadiths Text'])\n",
    "\n",
    "# Simpan dataframe ke dalam file CSV\n",
    "df.to_csv('hadiths_data.csv', index=False, encoding='utf-8')\n",
    "print(\"\\n=== Proses scraping selesai ===\")\n",
    "print(f\"Total {len(df)} hadis berhasil dikumpulkan dan disimpan ke dalam file hadiths_data.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "The data limited to 1000 row due to Gemini API Call limitation for synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('hadiths_data.csv')\n",
    "df[\"Hadiths Text\"] = df[\"Hadiths Text\"].astype(str).apply(lambda x: \" \".join(x.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \").split()))\n",
    "df.groupby('Rawi').count()\n",
    "df = df.groupby('Rawi').apply(lambda x: x.sample(n=85, random_state=1)).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ID, Extract Hadith's Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# Function to generate a unique ID based on multiple columns\n",
    "def generate_id(row):\n",
    "    unique_string = f\"{row['Rawi']}_{row['Chapter']}_{row['Reference']}_{row['Narator']}_{row['Hadiths Text']}\"\n",
    "    return hashlib.md5(unique_string.encode()).hexdigest()\n",
    "\n",
    "# Apply the function to each row to create a new 'ID' column\n",
    "df['ID'] = df.apply(generate_id, axis=1)\n",
    "\n",
    "# Move the 'ID' column to the first position\n",
    "columns = ['ID'] + [col for col in df.columns if col != 'ID']\n",
    "df = df[columns]\n",
    "\n",
    "# Display the DataFrame with the 'ID' column as the first column\n",
    "df.to_csv('hadiths_data_with_questions_final.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the number from the 'Reference' column\n",
    "def extract_hadith_number(reference):\n",
    "    if pd.isna(reference):\n",
    "        return np.nan\n",
    "    return reference.split()[-1]\n",
    "\n",
    "# Apply the function to create the 'Hadith Number' column\n",
    "df['Hadith Number'] = df['Reference'].apply(extract_hadith_number)\n",
    "\n",
    "# Move the 'Hadith Number' column to the 5th position\n",
    "columns = df.columns.tolist()\n",
    "columns.insert(4, columns.pop(columns.index('Hadith Number')))\n",
    "df = df[columns]\n",
    "\n",
    "# Display the DataFrame with the 'Hadith Number' column in the 5th position\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Question and Answer using GEMINI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='generation_log.txt',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('hadiths_data.csv')\n",
    "    logging.info(\"Successfully loaded 'hadiths_data.csv'\") # add log\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Error: 'hadiths_data.csv' not found.  Please ensure the file exists.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading 'hadiths_data.csv': {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Clean and preprocess the 'Hadiths Text' column\n",
    "try:\n",
    "    df[\"Hadiths Text\"] = df[\"Hadiths Text\"].astype(str).apply(\n",
    "        lambda x: \" \".join(x.replace(\"\\n\", \" \")\n",
    "                             .replace(\"\\t\", \" \")\n",
    "                             .replace(\"\\r\", \" \")\n",
    "                             .split()))\n",
    "    logging.info(\"Successfully preprocessed 'Hadiths Text' column.\")\n",
    "except KeyError as e:\n",
    "    logging.error(f\"Error: Missing column '{e}' in the dataset. Please check the column names.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error preprocessing 'Hadiths Text' column: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Optionally, sample the data to reduce the number of data points\n",
    "try:\n",
    "    df = df.groupby('Rawi').apply(\n",
    "        lambda x: x.sample(n=85, random_state=1)\n",
    "    ).reset_index(drop=True)\n",
    "    logging.info(\"Successfully sampled data by 'Rawi'.\")\n",
    "except KeyError as e:\n",
    "    logging.warning(f\"Column 'Rawi' not found. Sampling by 'Rawi' will be skipped.\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Error sampling data by 'Rawi': {e}. Sampling step will be skipped.\")\n",
    "    # Continue without sampling if there's an error\n",
    "    pass  # or consider setting `df` to original data:  df = pd.read_csv('hadiths_data.csv')\n",
    "\n",
    "# --- Gemini API Setup ---\n",
    "\n",
    "# Retrieve Google AI API Key from environment variables\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    logging.error(\"Error: GEMINI_API_KEY environment variable not set.\")\n",
    "    exit() # exit to not continue the code if there are no API_KEY\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Load the Gemini model\n",
    "try:\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")  # Or \"gemini-1.5-pro\" if available\n",
    "    logging.info(\"Successfully loaded Gemini model.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading Gemini model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Question Generation Function ---\n",
    "\n",
    "def generate_questions_batch(hadiths, indices, retries=3):\n",
    "    \"\"\"\n",
    "    Generates questions for a batch of hadiths using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        hadiths (list): A list of hadith texts.\n",
    "        indices (list): A list of indices corresponding to the hadiths in the original DataFrame.\n",
    "        retries (int): The number of retries if the API call fails.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated questions.  Returns error messages if generation fails.\n",
    "    \"\"\"\n",
    "    response_texts = []\n",
    "    for hadith, index in zip(hadiths, indices): # Use zip for iteration\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                prompt = f\"\"\"Generate a question in interrogative form, based on the following hadith as the answer:\\n\\n{hadith}\\n\\n\"\"\"\n",
    "                response = model.generate_content(prompt, generation_config=genai.types.GenerationConfig(\n",
    "                    max_output_tokens=128,\n",
    "                    temperature=0.8\n",
    "                ))\n",
    "\n",
    "                generated_text = response.text.strip() if response else \"Error: No response\"\n",
    "                print(f\"\\n[DEBUG] Row {index}\") # debugging\n",
    "                print(f\"[DEBUG] Generated Question: {generated_text}\\n\") # debugging\n",
    "                response_texts.append(generated_text)\n",
    "                logging.info(f\"Row {index} - Generated question: {generated_text[:50]}...\") # add log\n",
    "                break  # If successful, break the retry loop\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                error_message = str(e)\n",
    "                logging.error(f\"Row {index} - Attempt {attempt} - Error: {error_message}\")\n",
    "                if attempt == retries:\n",
    "                    response_texts.append(f\"Error after {retries} attempts: {error_message}\")\n",
    "                    break  # Break the loop if retries are exhausted\n",
    "                else:\n",
    "                    time.sleep(5)  # Wait before retrying\n",
    "\n",
    "    return response_texts\n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "\n",
    "# Set batch size and initialize question list\n",
    "batch_size = 7  # Adjust batch size based on API limits and performance\n",
    "questions = [\"\"] * len(df) # Initialize list with empty strings to match the dataframe length\n",
    "checkpoint_interval = 2  # Save progress every X batches\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    # Create a batch of hadiths and indices\n",
    "    batch_hadiths = df['Hadiths Text'][i:i + batch_size].tolist()\n",
    "    batch_indices = list(range(i, min(i + batch_size, len(df))))\n",
    "\n",
    "    # Generate questions for the batch\n",
    "    batch_questions = generate_questions_batch(batch_hadiths, batch_indices)\n",
    "\n",
    "    # Update the questions list with the generated questions\n",
    "    for idx, question in zip(batch_indices, batch_questions):\n",
    "        questions[idx] = question\n",
    "\n",
    "    # Introduce a random sleep interval to be kind to the API\n",
    "    sleep_duration = np.random.uniform(8, 10)  # Sleep for a random duration between 8 and 10 seconds\n",
    "    print(f\"Sleeping for {sleep_duration:.2f} seconds...\") # debug\n",
    "    time.sleep(sleep_duration)\n",
    "\n",
    "    # Save progress periodically\n",
    "    if (i // batch_size) % checkpoint_interval == 0 and i > 0:  # Save every checkpoint_interval batches, skip the very first iteration\n",
    "        df['Sample Question'] = questions\n",
    "        try:\n",
    "            df.to_csv('hadiths_data_with_questions_checkpoint.csv', index=False)\n",
    "            logging.info(f\"Checkpoint saved at batch {i // batch_size}. Rows processed: {i}\") # add log\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error saving checkpoint at batch {i // batch_size}: {e}\")\n",
    "\n",
    "# Save final results\n",
    "df['Sample Question'] = questions\n",
    "try:\n",
    "    df.to_csv('hadiths_data_with_questions_final.csv', index=False)\n",
    "    logging.info(\"Final results saved to 'hadiths_data_with_questions_final.csv'\") # add log\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving final results: {e}\")\n",
    "\n",
    "\n",
    "print(\"Question generation complete.\") # add confirmation print statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='generation_log.txt',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# --- API Key Setup ---\n",
    "\n",
    "# Retrieve Google AI API Key from environment variables\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    logging.error(\"Error: GEMINI_API_KEY environment variable not set.\")\n",
    "    exit() # exit to not continue the code if there are no API_KEY\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- Model Loading ---\n",
    "\n",
    "# Load the Gemini model\n",
    "try:\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")  # Or \"gemini-1.5-pro\" if available\n",
    "    logging.info(\"Successfully loaded Gemini model.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading Gemini model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('hadiths_data.csv')\n",
    "    logging.info(\"Successfully loaded 'hadiths_data.csv'\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Error: 'hadiths_data.csv' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading 'hadiths_data.csv': {e}\")\n",
    "    exit()\n",
    "\n",
    "# Ensure 'Sample Question' column exists. Create if missing and fill with empty string\n",
    "if 'Sample Question' not in df.columns:\n",
    "    df['Sample Question'] = \"\"\n",
    "    logging.warning(\"Column 'Sample Question' not found. Created and initialized with empty strings.\")\n",
    "\n",
    "# Clean and preprocess the 'Hadiths Text' column (If not already preprocessed)\n",
    "# This part is optional, if the cleaning already done\n",
    "try:\n",
    "    df[\"Hadiths Text\"] = df[\"Hadiths Text\"].astype(str).apply(\n",
    "        lambda x: \" \".join(x.replace(\"\\n\", \" \")\n",
    "                             .replace(\"\\t\", \" \")\n",
    "                             .replace(\"\\r\", \" \")\n",
    "                             .split()))\n",
    "    logging.info(\"Successfully preprocessed 'Hadiths Text' column.\")\n",
    "except KeyError as e:\n",
    "    logging.error(f\"Error: Missing column '{e}' in the dataset. Please check the column names.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error preprocessing 'Hadiths Text' column: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Question Generation Function ---\n",
    "\n",
    "def generate_questions_batch(hadiths, indices, retries=3):\n",
    "    \"\"\"\n",
    "    Generates questions for a batch of hadiths using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        hadiths (list): A list of hadith texts.\n",
    "        indices (list): A list of indices corresponding to the hadiths in the original DataFrame.\n",
    "        retries (int): The number of retries if the API call fails.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated questions. Returns error messages if generation fails.\n",
    "    \"\"\"\n",
    "    response_texts = []\n",
    "    for hadith, index in zip(hadiths, indices): # Use zip for iteration\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                prompt = f\"\"\"Generate a question in interrogative form, based on the following hadith as the answer:\\n\\n{hadith}\\n\\n\"\"\"\n",
    "                response = model.generate_content(prompt, generation_config=genai.types.GenerationConfig(\n",
    "                    max_output_tokens=128,\n",
    "                    temperature=0.8\n",
    "                ))\n",
    "\n",
    "                generated_text = response.text.strip() if response else \"Error: No response\"\n",
    "                print(f\"\\n[DEBUG] Row {index}\") # debugging\n",
    "                print(f\"[DEBUG] Generated Question: {generated_text}\\n\") # debugging\n",
    "                response_texts.append(generated_text)\n",
    "                logging.info(f\"Row {index} - Generated question: {generated_text[:50]}...\") # add log\n",
    "                break  # If successful, break the retry loop\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                error_message = str(e)\n",
    "                logging.error(f\"Row {index} - Attempt {attempt} - Error: {error_message}\")\n",
    "                if attempt == retries:\n",
    "                    response_texts.append(f\"Error after {retries} attempts: {error_message}\")\n",
    "                    break  # Break the loop if retries are exhausted\n",
    "                else:\n",
    "                    time.sleep(5)  # Wait before retrying\n",
    "\n",
    "    return response_texts\n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "\n",
    "# Set batch size and other parameters\n",
    "batch_size = 7  # Adjust based on API limits and performance\n",
    "checkpoint_interval = 2  # Save progress every X batches\n",
    "\n",
    "# Identify rows with null values in 'Sample Question'\n",
    "null_indices = df[df['Sample Question'].isnull() | (df['Sample Question'] == \"\")].index.tolist()  # Include empty strings too\n",
    "\n",
    "# Start processing only if there are rows to process\n",
    "if null_indices:\n",
    "    print(f\"Generating questions for {len(null_indices)} hadiths...\")\n",
    "    for i in range(0, len(null_indices), batch_size):\n",
    "        # Create a batch of hadiths and indices\n",
    "        batch_indices = null_indices[i:i + batch_size]\n",
    "        batch_hadiths = df.loc[batch_indices, 'Hadiths Text'].tolist()\n",
    "\n",
    "        # Generate questions for the batch\n",
    "        batch_questions = generate_questions_batch(batch_hadiths, batch_indices)\n",
    "\n",
    "        # Update the 'Sample Question' column with generated questions\n",
    "        for idx, question in zip(batch_indices, batch_questions):\n",
    "            df.loc[idx, 'Sample Question'] = question\n",
    "\n",
    "        # Introduce a random sleep interval\n",
    "        sleep_duration = np.random.uniform(9, 10) # Sleep a bit longer\n",
    "        print(f\"Sleeping for {sleep_duration:.2f} seconds...\") # debug\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "        # Save progress periodically\n",
    "        if (i // batch_size) % checkpoint_interval == 0 and i > 0:  # Skip first iteration\n",
    "            try:\n",
    "                df.to_csv('hadiths_data_with_questions_checkpoint.csv', index=False)\n",
    "                logging.info(f\"Checkpoint saved at batch {i // batch_size}. Rows processed: {i}\") # add log\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving checkpoint at batch {i // batch_size}: {e}\")\n",
    "else:\n",
    "    print(\"No rows with missing questions found. Skipping question generation.\")\n",
    "    logging.info(\"No rows with missing questions found. Skipping question generation.\")\n",
    "# --- Save Results ---\n",
    "try:\n",
    "    df.to_csv('hadiths_data_with_questions_final.csv', index=False)\n",
    "    logging.info(\"Final results saved to 'hadiths_data_with_questions_final.csv'\") # add log\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving final results: {e}\")\n",
    "# --- Display Result (Optional) ---\n",
    "print(\"\\n=== Final DataFrame ===\")\n",
    "print(df) # Print the dataframe\n",
    "\n",
    "print(\"Question generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Creation from dataset using ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('hadiths_data_with_question_and_answers_final.csv')\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"hadith_synthetic_rag_source\") \n",
    "\n",
    "# Create a collection\n",
    "collection = client.create_collection(name=\"hadith_synthetic_rag_source_complete\")\n",
    "\n",
    "# Load the pre-trained model with CUDA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "# Create documents from the DataFrame\n",
    "documents = [\n",
    "    f\"\"\"Rawi: {row['Rawi']}\\n\n",
    "    Chapter: {row['Chapter']}\\n\n",
    "    Reference: {row['Reference']}\\n\n",
    "    Hadith Number: {row['Hadith Number']}\\n\n",
    "    Narator: {row['Narator']}\\n\n",
    "    Hadith Text: {row['Hadiths Text']}\\n\n",
    "    Sample Question: {row['Sample Question']}\\n\n",
    "    Answer: {row['Synthetic Answer']}\\n\n",
    "    \"\"\"\n",
    "\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "ids = [f\"row_{i}\" for i in range(len(documents))]\n",
    "\n",
    "# Compute embeddings using CUDA\n",
    "embeddings = model.encode(documents, convert_to_tensor=True, device=device)\n",
    "\n",
    "# Add documents to the collection\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    ids=ids,\n",
    "    embeddings=embeddings.cpu().numpy()  # Convert embeddings to numpy array\n",
    ")\n",
    "\n",
    "# Debugging print\n",
    "print(f\"Number of documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the chatbot pipeline with gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from chromadb.utils import embedding_functions\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('hadiths_data_with_question_and_answers_final.csv')\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"hadith_synthetic_rag_source\") \n",
    "\n",
    "# Load the collection\n",
    "collection_name = \"hadith_synthetic_rag_source_complete\"\n",
    "collection = client.get_collection(collection_name)\n",
    "\n",
    "# Debugging print to verify the number of documents in the collection\n",
    "print(f\"Number of documents in collection: {collection.count()}\")\n",
    "\n",
    "# Model and Tokenizer Loading\n",
    "# model_name = \"google/flan-t5-base\"\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "retrieval_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "# Function to query the collection\n",
    "def query_collection(query, n_results):\n",
    "    # Compute the embedding for the query\n",
    "    query_embedding = retrieval_model.encode([query], convert_to_tensor=True, device=device).cpu().numpy()\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(query_embeddings=query_embedding, n_results=n_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate a response using the retrieved documents as context\n",
    "def generate_response(context, question):\n",
    "    prompt = f\"Please provide a short, well-structured answer and avoids repetition from context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = llm.generate(**inputs, max_length=2048, num_return_sequences=1, num_beams=5, temperature=0.9, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Main chatbot function with basic RAG\n",
    "def chatbot_response(user_query, top_k=2):\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    results = query_collection(user_query, top_k)\n",
    "    \n",
    "    # Step 2: Combine retrieved documents into context\n",
    "    documents = [doc for doc_list in results['documents'] for doc in doc_list]\n",
    "    combined_context = \"\\n\\n\".join(documents)\n",
    "    \n",
    "    # Step 3: Generate a response using the combined context\n",
    "    response = generate_response(combined_context, user_query)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Global variable to control the processing state\n",
    "stop_processing = False\n",
    "\n",
    "def chatbot(query, num_candidates):\n",
    "    global stop_processing\n",
    "    stop_processing = False  # Reset stop flag at the beginning of each query\n",
    "\n",
    "    # Jika query kosong, kembalikan pesan default\n",
    "    if not query.strip():\n",
    "        return \"Please ask a question about hadiths.\"\n",
    "    \n",
    "    # Lakukan retrieval dan generation dengan Speculative RAG\n",
    "    answer = chatbot_response(query, num_candidates)\n",
    "    \n",
    "    # Check if stop button was pressed\n",
    "    if stop_processing:\n",
    "        return \"Processing was stopped by the user.\"\n",
    "\n",
    "    # Format jawaban\n",
    "    if \"don't know\" in answer.lower() or \"not sure\" in answer.lower():\n",
    "        return \"Sorry. I don't have information about the hadiths related. It might be a dhoif, or maudhu, or I just don't have the knowledge.\"\n",
    "    else:\n",
    "        return answer\n",
    "\n",
    "def stop():\n",
    "    global stop_processing\n",
    "    stop_processing = True\n",
    "    return \"Processing stopped.\"\n",
    "\n",
    "# Buat Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Burhan AI\n",
    "        Assalamualaikum! I am Burhan AI, a chatbot that can help you find answers to your questions about hadiths. \n",
    "        \\n \n",
    "        Please note that this is a demo version and may not be perfect.\n",
    "        This chatbot is powered by the ChromaDB and Flan-T5-base models with RAG architecture.\n",
    "        Flan-T5-base is a small model and may not be as accurate as the bigger models.\n",
    "        If you have any feedback or suggestions, you can contact me at frendyrachman7@gmail.com\n",
    "        \\n\n",
    "        Jazakallah Khairan!\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        query_input = gr.Textbox(lines=2, placeholder=\"Enter your question here...\")\n",
    "        num_candidates_input = gr.Slider(minimum=1, maximum=10, value=2, step=1, label=\"Number of References\")\n",
    "        submit_button = gr.Button(\"Submit\")\n",
    "    \n",
    "    output_text = gr.Textbox(label=\"Response\")\n",
    "\n",
    "    submit_button.click(chatbot, inputs=[query_input, num_candidates_input], outputs=output_text)\n",
    "\n",
    "    # Add a button to stop processing\n",
    "    stop_button = gr.Button(\"Stop Processing\")\n",
    "    stop_output = gr.Textbox(visible=False)\n",
    "    stop_button.click(stop, inputs=[], outputs=stop_output)\n",
    "\n",
    "# Jalankan Gradio interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Preparation using unsloth and Llama 3.2 3b Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('hadiths_data_with_question_and_answers_final.csv')\n",
    "\n",
    "# Create the 'conversation' column with the specified format\n",
    "df['conversation'] = df.apply(lambda row: [\n",
    "    {\"from\": \"system\", \"value\": \"You are an AI assistant for hadiths. Please provide a response based on the hadith.\"},\n",
    "    {\"from\": \"human\", \"value\": row['Sample Question']},\n",
    "    {\"from\": \"system\", \"value\": row['Hadiths Text']},\n",
    "    {\"from\": \"gpt\", \"value\": row['Synthetic Answer']}\n",
    "], axis=1)\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.remove_columns(['Rawi', 'Chapter', 'Reference', 'Narator', 'Hadiths Text', 'Sample Question', 'Synthetic Answer', 'Hadith Number'])   # Remove unnecessary columns\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('hadiths_data_with_question_and_answers_final.csv')\n",
    "\n",
    "# Create the 'conversation' column with the specified format\n",
    "df['conversations'] = df.apply(lambda row: [\n",
    "    {\"from\": \"system\", \"value\": \"You are an AI assistant for hadiths. Please provide a response based on the hadith.\"},\n",
    "    {\"from\": \"human\", \"value\": row['Sample Question']},\n",
    "    {\"from\": \"gpt\", \"value\": row['Synthetic Answer']}\n",
    "], axis=1)\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)   \n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "max_seq_length = 2048 # Maximum sequence length for the model\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('hadiths_data_with_question_and_answers_final.csv')\n",
    "\n",
    "# Create the 'conversation' column with the specified format\n",
    "df['conversations'] = df.apply(lambda row: [\n",
    "    {\"from\": \"system\", \"value\": \"You are an AI assistant for hadiths. Please provide a response based on the hadith.\"},\n",
    "    {\"from\": \"human\", \"value\": row['Sample Question']},\n",
    "    {\"from\": \"gpt\", \"value\": row['Synthetic Answer']}\n",
    "], axis=1)\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)   \n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])\n",
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\") # Local saving\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "model.push_to_hub(\"your_name/burhan-ai-finetuned-llama-3.2-3b-i\", token=token) # Online saving\n",
    "tokenizer.push_to_hub(\"your_name/burhan-ai-finetuned-llama-3.2-3b-i\", token=token) # Online saving"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
